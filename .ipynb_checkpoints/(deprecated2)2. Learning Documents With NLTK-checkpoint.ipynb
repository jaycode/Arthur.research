{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Documents With NLTK\n",
    "\n",
    "Let's explore the possibility of learning the documents by creating a corpus for them in python's NLTK package. This is important to help Arthur learns [collocations](http://www.nltk.org/howto/collocations.html) (expressions with multiple words), so it knows when to split tokens with more than one word.\n",
    "\n",
    "The goal here is to split this text, for example:\n",
    "\n",
    "```\n",
    "Finest property in New Brunswick! Modern, luxurious, architectural home takes full advantage of center stage on Lake Utopia in St. George. Panoramic views of the lake, 27,000 sq.ft.under roof and 100+ acres of unspoiled natural beauty. Experience resort-style living with 3 homes, 2 tournament quality outdoor tennis courts, and 1 stadium quality indoor tennis court with state-of-the-art indoor stadium lighting, water park including 2 pools & wading pool with umbrella feature, beach volleyball court, baseball field, custom go-kart track, driving range, indoor basketball court, playground, private dock with boat lift and 3 private beaches. Main home offers expanses of glass flooding the interior with brilliant light, sleek contemporary design, dramatic master suite with custom shower/central tub showcasing unparalleled views, master lanai with drapery screening and built-in Jacuzzi. Two guest homes provide luxurious privacy for visitors enjoying this exquisite estate. Welcome to paradise!\n",
    "```\n",
    "\n",
    "Into this:\n",
    "\n",
    "```\n",
    "[('finest', ), 'property', 'in', '']\n",
    "```\n",
    "\n",
    "Then with human aid turn it into concepts:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        name: 'location',\n",
    "        values: ['new brunswick', 'st. george'],\n",
    "        hints: ['in']\n",
    "    },\n",
    "    {\n",
    "        name: 'features',\n",
    "        values: ['modern', 'luxurious', 'architectural', 'tennis courts', 'built-in jacuzzi', ...]\n",
    "    },\n",
    "    {\n",
    "        name: 'bulding size',\n",
    "        values: ['27,000 sq.ft'],\n",
    "        hints: ['under roof']\n",
    "    },\n",
    "    {\n",
    "        name: 'land size',\n",
    "        values: ['100+ acres'],\n",
    "        hints: ['unspoiled natural beauty']\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "## 1. Collocations in prebuilt corpus\n",
    "\n",
    "First we will calculate collocations in corpurs that came with NLTK, just to see if this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.genesis.words('english-web.txt'))\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are corpus words stored and used anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nltk.corpus.genesis.words('english-web.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, turns out we simply need to pass tokenized words into `BigramCollocationFinder.from_words()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building our own corpus from plaintext\n",
    "\n",
    "In the above, we inputted `nltk.corpus.genesis.words` into `BigramCollocationFinder`. How do we create our own corpus from some plaintext files? Let's explore it below. This was all from [this stackoverflow discussion](http://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# Let's create a corpus with 2 texts in different textfile.\n",
    "txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\n",
    "txt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\"\n",
    "corpus = [txt1,txt2]\n",
    "\n",
    "# Make new dir for the corpus.\n",
    "corpusdir = 'newcorpus/'\n",
    "if not os.path.isdir(corpusdir):\n",
    "    os.mkdir(corpusdir)\n",
    "\n",
    "# Output the files into the directory.\n",
    "filename = 0\n",
    "for text in corpus:\n",
    "    filename+=1\n",
    "    with open(corpusdir+str(filename)+'.txt','w') as fout:\n",
    "        print>>fout, text\n",
    "\n",
    "# Check that our corpus do exist and the files are correct.\n",
    "assert os.path.isdir(corpusdir)\n",
    "for infile, text in zip(sorted(os.listdir(corpusdir)),corpus):\n",
    "    assert open(corpusdir+infile,'r').read().strip() == text.strip()\n",
    "\n",
    "# Create a new corpus by specifying the parameters\n",
    "# (1) directory of the new corpus\n",
    "# (2) the fileids of the corpus\n",
    "# NOTE: in this case the fileids are simply the filenames.\n",
    "newcorpus = PlaintextCorpusReader('newcorpus/', '.*')\n",
    "\n",
    "import pdb\n",
    "# Access each file in the corpus.\n",
    "for infile in sorted(newcorpus.fileids()):\n",
    "    print infile # The fileids of each file.\n",
    "    fin = newcorpus.open(infile) # Opens the file.\n",
    "    print fin.read().strip() # Prints the content of the file\n",
    "    fin.close()\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access the plaintext; outputs pure string/basestring.\n",
    "print newcorpus.raw().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access paragraphs in the corpus. (list of list of list of strings)\n",
    "# NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and \n",
    "#       nltk.tokenize.word_tokenize.\n",
    "#\n",
    "# Each element in the outermost list is a paragraph, and\n",
    "# Each paragraph contains sentence(s), and\n",
    "# Each sentence contains token(s)\n",
    "print newcorpus.paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To access pargraphs of a specific fileid.\n",
    "print newcorpus.paras(newcorpus.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access sentences in the corpus. (list of list of strings)\n",
    "# NOTE: That the texts are flattened into sentences that contains tokens.\n",
    "print newcorpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To access sentences of a specific fileid.\n",
    "print newcorpus.sents(newcorpus.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access just tokens/words in the corpus. (list of strings)\n",
    "print newcorpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To access tokens of a specific fileid.\n",
    "print newcorpus.words(newcorpus.fileids()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collocations in custom corpus\n",
    "\n",
    "Putting the words from our custom corpus into `BigramAssocMeasures`, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    newcorpus.words())\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(newcorpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Next, we will look into implementing this with our documents.\n",
    "\n",
    "## 4. BigramAssocMeasures in extracted pdf documents\n",
    "\n",
    "We are going to follow the following pipeline:\n",
    "\n",
    "- Use ArthurReader to export documents into plaintexts.\n",
    "- Get words from them.\n",
    "- Use words to create collocations.\n",
    "- Use collocations in word splitting.\n",
    "\n",
    "The reason ArthurReader is needed when exporting documents is due to the bolded texts issue in pdf i.e. **A Text** would be written as \"A TextA TextA TextA Text\" if we just extract them as they are (when extracted from ArthurDocument, that is). Ideally later on we should keep bold information as feature, but for now let's just remove the duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Exporting to plaintexts\n",
    "\n",
    "This section attempts to extract documents and remove duplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparing required modules.\n",
    "import os\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "import sys\n",
    "import inspect\n",
    "base_path = os.path.realpath(\n",
    "    os.path.abspath(\n",
    "        os.path.join(\n",
    "            os.path.split(\n",
    "                inspect.getfile(\n",
    "                    inspect.currentframe()\n",
    "                )\n",
    "            )[0],\n",
    "            '..'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "sys.path.append(base_path)\n",
    "\n",
    "class ListTable(list):\n",
    "    \"\"\" Overridden list class which takes a 2-dimensional list of \n",
    "        the form [[1,2,3],[4,5,6]], and renders an HTML Table in \n",
    "        IPython Notebook. \"\"\"\n",
    "    \n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col.encode('utf-8')))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from libs.arthur import ArthurDocument\n",
    "from zipfile import ZipFile\n",
    "from libs.arthur.errors import BatchReadingError\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "from libs.arthur import ArthurReader\n",
    "from libs.arthur.cluster import DefaultCluster\n",
    "\n",
    "# These are prototypes for ArthurReader's methods.\n",
    "\n",
    "def correct_block(block, return_details=False):\n",
    "    \"\"\"Corrects block elements.\n",
    "    \n",
    "    Args:\n",
    "        block(np.array): Block to correct\n",
    "        \n",
    "        return_details(bool): If True, return list instead of only corrected block. This list\n",
    "                              contains details needed for debugging:\n",
    "                              - removed features\n",
    "                              - added features\n",
    "                              Defaults to False\n",
    "    \"\"\"\n",
    "    fxid = ArthurDocument.get_feature_id('x')\n",
    "    fyid = ArthurDocument.get_feature_id('y')\n",
    "    positions = block[:,[fxid,fyid]]\n",
    "    tree = cKDTree(positions)\n",
    "\n",
    "    # Removes duplicate elements that are close together\n",
    "    radius = 0.4\n",
    "    neighbors = tree.query_ball_point(positions, radius)\n",
    "    neighbors = np.unique(neighbors)\n",
    "    # This returns numpy array like:\n",
    "    # [[0, 13, 26, 39] [1, 14, 27, 40] [5, 31, 44, 18] [11, 24, 37, 50]\n",
    "    # [16, 29, 42, 3] [17, 30, 43, 4] [21, 8, 34, 47] [22, 35, 48, 9]\n",
    "    # [32, 45, 19, 6] [36, 23, 10, 49] [38, 12, 25, 51] [41, 28, 2, 15]\n",
    "    # [46, 33, 7, 20] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62]\n",
    "    # [63] [64]]\n",
    "    #\n",
    "    # Which we will then remove duplicates e.g. remove index 13, 26, 39, 14, 27, etc.\n",
    "    removed = []\n",
    "    for n in neighbors:\n",
    "        removed.extend(np.sort(n)[1:])\n",
    "    cblock = np.delete(block, removed, axis=0)\n",
    "    \n",
    "    if return_details:\n",
    "        return (cblock, removed)\n",
    "    else:\n",
    "        return cblock\n",
    "    \n",
    "def create_corpus(zip_path, corpus_dir, batch_size=100, start_batch=0, stdout=None, recreate=False):\n",
    "    \"\"\"Create corpus from zip file. A corpus is basically just a list of text files.\n",
    "    Args:\n",
    "        zip_path(str):    Path of zip file to load.\n",
    "        corpus_dir(str):  Path to corpus dir where the files will be written into.\n",
    "        batch_size(int):  Size of batch to be processed. If we have one million documents,\n",
    "                          we'd want to process them in batches. Defaults to 100.\n",
    "        start_batch(int): When an error happens in batch processing, reader will return\n",
    "                          index of the last batch processed. enter that index value to\n",
    "                          start processing from that batch index. Defaults to 0.\n",
    "        stdout(Object):   Pass sys.stdout to print progress, or pass any object with `write`\n",
    "                          method to pass printed progress to it.\n",
    "        recreate(bool):   Recreate files?\n",
    "    \"\"\"\n",
    "    zipfile = ZipFile(zip_path, 'r')\n",
    "    namelist = zipfile.namelist()\n",
    "    jobs_total = len(namelist)\n",
    "    jobs_left = jobs_total - start_batch*batch_size\n",
    "    \n",
    "    def process_batch(zipfile, corpus_dir, batch, total, counter=0):\n",
    "        for docname in batch:\n",
    "            counter += 1\n",
    "            filename = os.path.join(corpus_dir, docname+'.txt')\n",
    "            if os.path.isfile(filename):\n",
    "                stdout.write(\"%s already exists (%i/%i)\\n\" % (docname, counter, total))\n",
    "            else:\n",
    "                content = zipfile.read(docname)\n",
    "                stdout.write(\"processing %s (%i/%i)\\n\" % (docname, counter, total))\n",
    "                document = ArthurDocument(content, name=docname)\n",
    "                texts = get_texts(document, stopwords)\n",
    "                if len(texts) > 0:\n",
    "                    if not os.path.isdir(corpus_dir):\n",
    "                        os.mkdir(corpus_dir)\n",
    "\n",
    "                    with open(filename,'w') as fout:\n",
    "                        for text in texts:\n",
    "                            print>>fout, text\n",
    "                else:\n",
    "                    stdout.write(\"    empty text! moving on...\\n\")\n",
    "    \n",
    "    def get_texts(document):\n",
    "        blocks = DefaultCluster.create_blocks(document)\n",
    "        \n",
    "        texts = []\n",
    "        for idx, block in enumerate(blocks):\n",
    "            cblock = ArthurReader.correct_block(block)\n",
    "            texts.append(document.get_text(cblock))\n",
    "        return texts\n",
    "\n",
    "    while jobs_left > 0:\n",
    "        job_start = jobs_total - jobs_left\n",
    "        job_end = job_start + batch_size\n",
    "        batch = namelist[job_start:job_end]\n",
    "        process_batch(zipfile, corpus_dir, batch, jobs_total, job_start)\n",
    "        jobs_left -= batch_size\n",
    "\n",
    "    zipfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1123680.pdf already exists (1/300)\n",
      "V1119135.pdf already exists (2/300)\n",
      "V1102480.pdf already exists (3/300)\n",
      "V1123741.pdf already exists (4/300)\n",
      "V1119430.pdf already exists (5/300)\n",
      "V1122760.pdf already exists (6/300)\n",
      "V1123301.pdf already exists (7/300)\n",
      "F1439898.pdf already exists (8/300)\n",
      "V1116097.pdf already exists (9/300)\n",
      "V1124383.pdf already exists (10/300)\n",
      "V1120838.pdf already exists (11/300)\n",
      "V1117770.pdf already exists (12/300)\n",
      "V1104999.pdf already exists (13/300)\n",
      "V1099142.pdf already exists (14/300)\n",
      "V1123148.pdf already exists (15/300)\n",
      "V1081878.pdf already exists (16/300)\n",
      "V1104283.pdf already exists (17/300)\n",
      "V1114927.pdf already exists (18/300)\n",
      "V1099185.pdf already exists (19/300)\n",
      "V1106593.pdf already exists (20/300)\n",
      "F1440892.pdf already exists (21/300)\n",
      "V1084554.pdf already exists (22/300)\n",
      "V1121393.pdf already exists (23/300)\n",
      "V1114732.pdf already exists (24/300)\n",
      "348906.pdf already exists (25/300)\n",
      "V1123093.pdf already exists (26/300)\n",
      "V1125886.pdf already exists (27/300)\n",
      "V1106471.pdf already exists (28/300)\n",
      "V1123295.pdf already exists (29/300)\n",
      "F1437417.pdf already exists (30/300)\n",
      "V1113765.pdf already exists (31/300)\n",
      "V1069495.pdf already exists (32/300)\n",
      "373787.pdf already exists (33/300)\n",
      "F1428720.pdf already exists (34/300)\n",
      "385472.pdf already exists (35/300)\n",
      "F1430137.pdf already exists (36/300)\n",
      "V1101209.pdf already exists (37/300)\n",
      "V1120451.pdf already exists (38/300)\n",
      "V1097237.pdf already exists (39/300)\n",
      "10091948.pdf already exists (40/300)\n",
      "F1433697.pdf already exists (41/300)\n",
      "10090774.pdf already exists (42/300)\n",
      "V1119133.pdf already exists (43/300)\n",
      "V1122664.pdf already exists (44/300)\n",
      "V1115900.pdf already exists (45/300)\n",
      "351454.pdf already exists (46/300)\n",
      "V1099419.pdf already exists (47/300)\n",
      "V1118264.pdf already exists (48/300)\n",
      "V1118949.pdf already exists (49/300)\n",
      "V1125756.pdf already exists (50/300)\n",
      "V1114934.pdf already exists (51/300)\n",
      "V1112094.pdf already exists (52/300)\n",
      "V1117596.pdf already exists (53/300)\n",
      "V1097961.pdf already exists (54/300)\n",
      "E3366096.pdf already exists (55/300)\n",
      "V1124278.pdf already exists (56/300)\n",
      "V1114166.pdf already exists (57/300)\n",
      "V1097187.pdf already exists (58/300)\n",
      "V1109592.pdf already exists (59/300)\n",
      "10099383.pdf already exists (60/300)\n",
      "V1095804.pdf already exists (61/300)\n",
      "V1122293.pdf already exists (62/300)\n",
      "V1104995.pdf already exists (63/300)\n",
      "V1118241.pdf already exists (64/300)\n",
      "V1109058.pdf already exists (65/300)\n",
      "V1111616.pdf already exists (66/300)\n",
      "V1114153.pdf already exists (67/300)\n",
      "F1441174.pdf already exists (68/300)\n",
      "V1122020.pdf already exists (69/300)\n",
      "V1117553.pdf already exists (70/300)\n",
      "F1440272.pdf already exists (71/300)\n",
      "V1123500.pdf already exists (72/300)\n",
      "V1096160.pdf already exists (73/300)\n",
      "V1104242.pdf already exists (74/300)\n",
      "V1067018.pdf already exists (75/300)\n",
      "V1086138.pdf already exists (76/300)\n",
      "V1080274.pdf already exists (77/300)\n",
      "V1097513.pdf already exists (78/300)\n",
      "V1113467.pdf already exists (79/300)\n",
      "V1102318.pdf already exists (80/300)\n",
      "V1112391.pdf already exists (81/300)\n",
      "V1107111.pdf already exists (82/300)\n",
      "F1436762.pdf already exists (83/300)\n",
      "V1121800.pdf already exists (84/300)\n",
      "C4006709.pdf already exists (85/300)\n",
      "V1124015.pdf already exists (86/300)\n",
      "V1114000.pdf already exists (87/300)\n",
      "10095140.pdf already exists (88/300)\n",
      "V1106596.pdf already exists (89/300)\n",
      "V1118040.pdf already exists (90/300)\n",
      "V1124059.pdf already exists (91/300)\n",
      "V1105289.pdf already exists (92/300)\n",
      "V1125072.pdf already exists (93/300)\n",
      "V1088335.pdf already exists (94/300)\n",
      "V1125023.pdf already exists (95/300)\n",
      "V1107713.pdf already exists (96/300)\n",
      "V1092694.pdf already exists (97/300)\n",
      "V1125832.pdf already exists (98/300)\n",
      "V1121981.pdf already exists (99/300)\n",
      "V1097093.pdf already exists (100/300)\n",
      "V1119118.pdf already exists (101/300)\n",
      "V1120987.pdf already exists (102/300)\n",
      "F1439887.pdf already exists (103/300)\n",
      "351903.pdf already exists (104/300)\n",
      "V1116019.pdf already exists (105/300)\n",
      "V1105177.pdf already exists (106/300)\n",
      "F1430272.pdf already exists (107/300)\n",
      "V1125447.pdf already exists (108/300)\n",
      "V1097285.pdf already exists (109/300)\n",
      "V1102177.pdf already exists (110/300)\n",
      "V1117659.pdf already exists (111/300)\n",
      "V1122193.pdf already exists (112/300)\n",
      "V1102548.pdf already exists (113/300)\n",
      "V1125794.pdf already exists (114/300)\n",
      "V1120593.pdf already exists (115/300)\n",
      "V1072991.pdf already exists (116/300)\n",
      "V1117056.pdf already exists (117/300)\n",
      "V1124747.pdf already exists (118/300)\n",
      "V1111101.pdf already exists (119/300)\n",
      "V1123523.pdf already exists (120/300)\n",
      "V1082668.pdf already exists (121/300)\n",
      "V981606.pdf already exists (122/300)\n",
      "V1114898.pdf already exists (123/300)\n",
      "F1442790.pdf already exists (124/300)\n",
      "V1111985.pdf already exists (125/300)\n",
      "V1108381.pdf already exists (126/300)\n",
      "V1123105.pdf already exists (127/300)\n",
      "V1121372.pdf already exists (128/300)\n",
      "E3313697.pdf already exists (129/300)\n",
      "V1123831.pdf already exists (130/300)\n",
      "V1117454.pdf already exists (131/300)\n",
      "V1114525.pdf already exists (132/300)\n",
      "V1104291.pdf already exists (133/300)\n",
      "E3391551.pdf already exists (134/300)\n",
      "V1104287.pdf already exists (135/300)\n",
      "10099708.pdf already exists (136/300)\n",
      "10093137.pdf already exists (137/300)\n",
      "SJ152241.pdf already exists (138/300)\n",
      "13070000.pdf already exists (139/300)\n",
      "41257197.pdf already exists (140/300)\n",
      "00390948.pdf already exists (141/300)\n",
      "60088317.pdf already exists (142/300)\n",
      "C4001601.pdf already exists (143/300)\n",
      "C4008847.pdf already exists (144/300)\n",
      "C4011862.pdf already exists (145/300)\n",
      "C3647620.pdf already exists (146/300)\n",
      "C4000958.pdf already exists (147/300)\n",
      "C4014364.pdf already exists (148/300)\n",
      "C4006483.pdf already exists (149/300)\n",
      "C3640664.pdf already exists (150/300)\n",
      "C4006332.pdf already exists (151/300)\n",
      "V1041596.pdf already exists (152/300)\n",
      "V1101775.pdf already exists (153/300)\n",
      "V1121558.pdf already exists (154/300)\n",
      "V1101014.pdf already exists (155/300)\n",
      "V1116606.pdf already exists (156/300)\n",
      "V1119028.pdf already exists (157/300)\n",
      "V1117600.pdf already exists (158/300)\n",
      "V1117508.pdf already exists (159/300)\n",
      "V1104865.pdf already exists (160/300)\n",
      "V1119997.pdf already exists (161/300)\n",
      "F1434302.pdf already exists (162/300)\n",
      "10099650.pdf already exists (163/300)\n",
      "349119.pdf already exists (164/300)\n",
      "V1117985.pdf already exists (165/300)\n",
      "V1117956.pdf already exists (166/300)\n",
      "V1078325.pdf already exists (167/300)\n",
      "V1096647.pdf already exists (168/300)\n",
      "V1116023.pdf already exists (169/300)\n",
      "V1122200.pdf already exists (170/300)\n",
      "F1440036.pdf already exists (171/300)\n",
      "V1117149.pdf already exists (172/300)\n",
      "V1076667.pdf already exists (173/300)\n",
      "V1115083.pdf already exists (174/300)\n",
      "V1116333.pdf already exists (175/300)\n",
      "V1119114.pdf already exists (176/300)\n",
      "V1121557.pdf already exists (177/300)\n",
      "V1121006.pdf already exists (178/300)\n",
      "V1077012.pdf already exists (179/300)\n",
      "F1439755.pdf already exists (180/300)\n",
      "392606.pdf already exists (181/300)\n",
      "F1433307.pdf already exists (182/300)\n",
      "10090865.pdf already exists (183/300)\n",
      "V1125746.pdf already exists (184/300)\n",
      "V1097312.pdf already exists (185/300)\n",
      "C4000662.pdf already exists (186/300)\n",
      "V1079038.pdf already exists (187/300)\n",
      "V1119498.pdf already exists (188/300)\n",
      "V1124248.pdf already exists (189/300)\n",
      "V1105996.pdf already exists (190/300)\n",
      "V1119009.pdf already exists (191/300)\n",
      "F1422736.pdf already exists (192/300)\n",
      "V1108240.pdf already exists (193/300)\n",
      "V1086595.pdf already exists (194/300)\n",
      "V1106553.pdf already exists (195/300)\n",
      "F1433545.pdf already exists (196/300)\n",
      "V1117166.pdf already exists (197/300)\n",
      "V1115935.pdf already exists (198/300)\n",
      "V1123902.pdf already exists (199/300)\n",
      "C3652931.pdf already exists (200/300)\n",
      "V1103145.pdf already exists (201/300)\n",
      "V1080575.pdf already exists (202/300)\n",
      "V1100978.pdf already exists (203/300)\n",
      "V1120732.pdf already exists (204/300)\n",
      "V1119523.pdf already exists (205/300)\n",
      "349860.pdf already exists (206/300)\n",
      "V1121207.pdf already exists (207/300)\n",
      "N3183480.pdf already exists (208/300)\n",
      "N3202699.pdf already exists (209/300)\n",
      "C3120190.pdf already exists (210/300)\n",
      "C3104853.pdf already exists (211/300)\n",
      "W3124013.pdf already exists (212/300)\n",
      "W3199234.pdf already exists (213/300)\n",
      "N3203701.pdf already exists (214/300)\n",
      "N3211853.pdf already exists (215/300)\n",
      "521930797.pdf already exists (216/300)\n",
      "C3140671.pdf already exists (217/300)\n",
      "H3140708.pdf already exists (218/300)\n",
      "C3185841.pdf already exists (219/300)\n",
      "N3162073.pdf already exists (220/300)\n",
      "C3214787.pdf already exists (221/300)\n",
      "W2946072.pdf already exists (222/300)\n",
      "N3214580.pdf already exists (223/300)\n",
      "C3153538.pdf already exists (224/300)\n",
      "N3078731.pdf already exists (225/300)\n",
      "C3211401.pdf already exists (226/300)\n",
      "520940126.pdf already exists (227/300)\n",
      "N3130306.pdf already exists (228/300)\n",
      "N3173212.pdf already exists (229/300)\n",
      "N3171343.pdf already exists (230/300)\n",
      "W3093246.pdf already exists (231/300)\n",
      "W3209997.pdf already exists (232/300)\n",
      "C3166960.pdf already exists (233/300)\n",
      "N3139956.pdf already exists (234/300)\n",
      "481960135.pdf already exists (235/300)\n",
      "C4006818.pdf already exists (236/300)\n",
      "E3402091.pdf already exists (237/300)\n",
      "1409195.pdf already exists (238/300)\n",
      "C3174921.pdf already exists (239/300)\n",
      "C4012236.pdf already exists (240/300)\n",
      "V1120698.pdf already exists (241/300)\n",
      "QR21406417.pdf already exists (242/300)\n",
      "V1113428.pdf already exists (243/300)\n",
      "V1097673.pdf already exists (244/300)\n",
      "V1122559.pdf already exists (245/300)\n",
      "V1115392.pdf already exists (246/300)\n",
      "V1102277.pdf already exists (247/300)\n",
      "V1114539.pdf already exists (248/300)\n",
      "V1115508.pdf already exists (249/300)\n",
      "V1123498.pdf already exists (250/300)\n",
      "V1088985.pdf already exists (251/300)\n",
      "V1066359.pdf already exists (252/300)\n",
      "V1124792.pdf already exists (253/300)\n",
      "V1122834.pdf already exists (254/300)\n",
      "V1120416.pdf already exists (255/300)\n",
      "F1429622.pdf already exists (256/300)\n",
      "V1095995.pdf already exists (257/300)\n",
      "F1439155.pdf already exists (258/300)\n",
      "F1439706.pdf already exists (259/300)\n",
      "353393.pdf already exists (260/300)\n",
      "V1118376.pdf already exists (261/300)\n",
      "348608.pdf already exists (262/300)\n",
      "348418.pdf already exists (263/300)\n",
      "F1426476.pdf already exists (264/300)\n",
      "V1023291.pdf already exists (265/300)\n",
      "V1057605.pdf already exists (266/300)\n",
      "V906290.pdf already exists (267/300)\n",
      "10085286.pdf already exists (268/300)\n",
      "10092912.pdf already exists (269/300)\n",
      "10086622.pdf already exists (270/300)\n",
      "10094899.pdf already exists (271/300)\n",
      "10094151.pdf already exists (272/300)\n",
      "10097297.pdf already exists (273/300)\n",
      "10090891.pdf already exists (274/300)\n",
      "10086107.pdf already exists (275/300)\n",
      "C3646135.pdf already exists (276/300)\n",
      "C4007280.pdf already exists (277/300)\n",
      "C3653337.pdf already exists (278/300)\n",
      "C3652657.pdf already exists (279/300)\n",
      "C3618078.pdf already exists (280/300)\n",
      "C3645887.pdf already exists (281/300)\n",
      "05337591.pdf already exists (282/300)\n",
      "processing 342920.docx (283/300)\n",
      "    empty text! moving on...\n",
      "processing 155317.docx (284/300)\n",
      "    empty text! moving on...\n",
      "processing 152912.docx (285/300)\n",
      "    empty text! moving on...\n",
      "processing 131776.docx (286/300)\n",
      "    empty text! moving on...\n",
      "processing 11758.docx (287/300)\n",
      "    empty text! moving on...\n",
      "V1121465.pdf already exists (288/300)\n",
      "V1103234.pdf already exists (289/300)\n",
      "10099505.pdf already exists (290/300)\n",
      "F1418810.pdf already exists (291/300)\n",
      "V1125602.pdf already exists (292/300)\n",
      "V1109972.pdf already exists (293/300)\n",
      "V1121707.pdf already exists (294/300)\n",
      "V1123287.pdf already exists (295/300)\n",
      "V1124814.pdf already exists (296/300)\n",
      "V1115519.pdf already exists (297/300)\n",
      "V1122295.pdf already exists (298/300)\n",
      "V1120680.pdf already exists (299/300)\n",
      "V1103720.pdf already exists (300/300)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.collocations import *\n",
    "from libs.arthur.errors import BatchReadingError\n",
    "\n",
    "zip_path = 'DRAS_sample_v1_20150605.zip'\n",
    "corpus_dir = 'corpus'\n",
    "try:\n",
    "    create_corpus(zip_path, corpus_dir, stdout=sys.stdout)        \n",
    "except BatchReadingError as e:\n",
    "    print(e.msg)\n",
    "    print('last batch was: %i' % e.last_batch)\n",
    "    # Maybe do some retries here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Text' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-365d0b1e36b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Text' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "text = nltk.Text(corpus.words())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British Columbia; http ://; http www; :// www; www realtor; Single\n",
      "Family; Walk Score; Real Estate; aspx PropertyId; propertyDetails\n",
      "aspx; Vancouver British; Residential Single; Data provided; displayed\n",
      "believed; warranties representations; independently verified;\n",
      "representations made; Show measurements; information displayed; kind\n",
      "Data; Bathrooms Total; Floor Space; Title Freehold; measurements\n",
      "Imperial; Real Board; Estate Board; 000 Listing; Land Size; Property\n",
      "Single; Fax 604; Style Detached; Building House; Car Dependent;\n",
      "Appliances Included; Property Type; Description Building; Score Walk;\n",
      "Space sqft; Basement Unknown; Board Greater; Building Type; Type\n",
      "House; Amenities Nearby; Description Type; Score Car; Fire Protection;\n",
      "Type Single; Details Nearby; Greater http; Type Family; Basement\n",
      "Features; West Vancouver; House Built; Basement Type; Details\n",
      "Amenities; Greater Vancouver; Personal Real; Real Corporation;\n",
      "Architecture Style; Estate Corporation; Personal Estate; Measurements\n",
      "available; Parking Type; Bedroom Bedroom; Protection system; Family\n",
      "Size; Protection Security; Building Architecture; Features Unknown;\n",
      "Size sqft; Land Frontage; Fire Security; Security system; Attached\n",
      "garage; Private setting; Back http; Back ://; ... Page; R13 image;\n",
      "image R13; R14 image; R15 image; image R14; image R15; 2015 R36;\n",
      "Single Land; R16 image; image R16; Family Land; R17 image; image R17;\n",
      "R18 image; image R18; Level Bathrooms; R19 image; image R19; 2015 R10;\n",
      "R20 image; image R20; Exterior Finish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collocations(100)\n",
    "len(text._collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "corpus = PlaintextCorpusReader(corpus_dir, '.*')\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_words(corpus.words())\n",
    "trigram_finder = TrigramCollocationFinder.from_words(corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>word1</td><td>word2</td></tr><tr><td>+.</td><td>Learn</td></tr><tr><td>/#</td><td>tab</td></tr><tr><td>0096</td><td>MARK</td></tr><tr><td>0702</td><td>Raymar</td></tr><tr><td>0928</td><td>Prudential</td></tr><tr><td>10171</td><td>Saskatchewan</td></tr><tr><td>1020</td><td>Austin</td></tr><tr><td>10X10</td><td>36X18</td></tr><tr><td>1206</td><td>CENTRE</td></tr><tr><td>1716</td><td>Craig</td></tr><tr><td>1726</td><td>JAMIE</td></tr><tr><td>179</td><td>Davie</td></tr><tr><td>17A</td><td>)|</td></tr><tr><td>1879</td><td>origins</td></tr><tr><td>2271</td><td>BLOOR</td></tr><tr><td>22ND</td><td>SIDEROAD</td></tr><tr><td>2514</td><td>180th</td></tr><tr><td>282</td><td>Gibsons</td></tr><tr><td>2X16</td><td>GUEST</td></tr><tr><td>311</td><td>NEILL</td></tr><tr><td>3115</td><td>KELLER</td></tr><tr><td>400sqft</td><td>3bdrm</td></tr><tr><td>5140</td><td>METRAL</td></tr><tr><td>670</td><td>Parkside</td></tr><tr><td>6Gb</td><td>Rng</td></tr><tr><td>711A</td><td>Woodstock</td></tr><tr><td>7203</td><td>Nina</td></tr><tr><td>7260</td><td>Homeland</td></tr><tr><td>7319</td><td>TEHRA</td></tr><tr><td>7330</td><td>1039</td></tr><tr><td>748</td><td>LWH</td></tr><tr><td>7694</td><td>ISLINGTON</td></tr><tr><td>7828</td><td>Prompton</td></tr><tr><td>836</td><td>sfmn</td></tr><tr><td>8812</td><td>RoyalLePage</td></tr><tr><td>AIR</td><td>CONDITIONING</td></tr><tr><td>ALKERTON</td><td>BROKER</td></tr><tr><td>ANDERSON</td><td>OEY</td></tr><tr><td>ANDREA</td><td>LIANG</td></tr><tr><td>ATTENTION</td><td>BUILDERS</td></tr><tr><td>Agro</td><td>Tourism</td></tr><tr><td>Amirmohsen</td><td>Hamzehali</td></tr><tr><td>Andrea</td><td>Kavanagh</td></tr><tr><td>Anthony</td><td>Henday</td></tr><tr><td>Antifaev</td><td>Jon</td></tr><tr><td>Approved</td><td>Footprint</td></tr><tr><td>Arthur</td><td>Chow</td></tr><tr><td>Aster</td><td>Cucine</td></tr><tr><td>Atefeh</td><td>Abbaspour</td></tr><tr><td>BARRY</td><td>COHEN</td></tr><tr><td>BEEN</td><td>AVAILABLE</td></tr><tr><td>BERNARD</td><td>LEUNG</td></tr><tr><td>BETTER</td><td>THAN</td></tr><tr><td>BRIGHT</td><td>VERSATILE</td></tr><tr><td>BY</td><td>RENOWNED</td></tr><tr><td>Barton</td><td>Myers</td></tr><tr><td>Bayview</td><td>Glen</td></tr><tr><td>Beachfront</td><td>Beauty</td></tr><tr><td>Berwick</td><td>whose</td></tr><tr><td>Bianco</td><td>Neve</td></tr><tr><td>Bio</td><td>Flame</td></tr><tr><td>Black</td><td>African</td></tr><tr><td>Blair</td><td>Gunn</td></tr><tr><td>Bob</td><td>Ferguson</td></tr><tr><td>Bows</td><td>archictects</td></tr><tr><td>Brian</td><td>Rybchinsky</td></tr><tr><td>Burn</td><td>Cktp</td></tr><tr><td>CANWEST</td><td>242</td></tr><tr><td>CARETAKER</td><td>ACC</td></tr><tr><td>CELLER</td><td>10X10</td></tr><tr><td>CERC</td><td>Relocation</td></tr><tr><td>CHARLENE</td><td>ALKERTON</td></tr><tr><td>CLASS</td><td>ART</td></tr><tr><td>COURT</td><td>ORDERED</td></tr><tr><td>COVETABLE</td><td>KERRISDALE</td></tr><tr><td>CP</td><td>Rail</td></tr><tr><td>CROWN</td><td>JEWEL</td></tr><tr><td>Cambrian</td><td>black</td></tr><tr><td>Cape</td><td>Cod</td></tr><tr><td>Careful</td><td>consideration</td></tr><tr><td>Carey</td><td>Mudford</td></tr><tr><td>Carissa</td><td>Siy</td></tr><tr><td>Carpe</td><td>diem</td></tr><tr><td>Casual</td><td>Gatherings</td></tr><tr><td>Cecilia</td><td>Xie</td></tr><tr><td>Celene</td><td>Cheang</td></tr><tr><td>Centennial</td><td>Parkway</td></tr><tr><td>Ch</td><td>u</td></tr><tr><td>Chebucto</td><td>Head</td></tr><tr><td>Childrens</td><td>Lottery</td></tr><tr><td>Christina</td><td>Watts</td></tr><tr><td>Circ</td><td>Drwy</td></tr><tr><td>Clarence</td><td>Debelle</td></tr><tr><td>Clay</td><td>Brickwork</td></tr><tr><td>Climate</td><td>Controlled</td></tr><tr><td>Coffee</td><td>Machine</td></tr><tr><td>Collectors</td><td>boaths</td></tr><tr><td>Communities</td><td>C1</td></tr><tr><td>Continental</td><td>Eastern</td></tr><tr><td>Craig</td><td>Doherty</td></tr></table>"
      ],
      "text/plain": [
       "[['word1', 'word2'],\n",
       " (u'+.', u'Learn'),\n",
       " (u'/#', u'tab'),\n",
       " (u'0096', u'MARK'),\n",
       " (u'0702', u'Raymar'),\n",
       " (u'0928', u'Prudential'),\n",
       " (u'10171', u'Saskatchewan'),\n",
       " (u'1020', u'Austin'),\n",
       " (u'10X10', u'36X18'),\n",
       " (u'1206', u'CENTRE'),\n",
       " (u'1716', u'Craig'),\n",
       " (u'1726', u'JAMIE'),\n",
       " (u'179', u'Davie'),\n",
       " (u'17A', u')|'),\n",
       " (u'1879', u'origins'),\n",
       " (u'2271', u'BLOOR'),\n",
       " (u'22ND', u'SIDEROAD'),\n",
       " (u'2514', u'180th'),\n",
       " (u'282', u'Gibsons'),\n",
       " (u'2X16', u'GUEST'),\n",
       " (u'311', u'NEILL'),\n",
       " (u'3115', u'KELLER'),\n",
       " (u'400sqft', u'3bdrm'),\n",
       " (u'5140', u'METRAL'),\n",
       " (u'670', u'Parkside'),\n",
       " (u'6Gb', u'Rng'),\n",
       " (u'711A', u'Woodstock'),\n",
       " (u'7203', u'Nina'),\n",
       " (u'7260', u'Homeland'),\n",
       " (u'7319', u'TEHRA'),\n",
       " (u'7330', u'1039'),\n",
       " (u'748', u'LWH'),\n",
       " (u'7694', u'ISLINGTON'),\n",
       " (u'7828', u'Prompton'),\n",
       " (u'836', u'sfmn'),\n",
       " (u'8812', u'RoyalLePage'),\n",
       " (u'AIR', u'CONDITIONING'),\n",
       " (u'ALKERTON', u'BROKER'),\n",
       " (u'ANDERSON', u'OEY'),\n",
       " (u'ANDREA', u'LIANG'),\n",
       " (u'ATTENTION', u'BUILDERS'),\n",
       " (u'Agro', u'Tourism'),\n",
       " (u'Amirmohsen', u'Hamzehali'),\n",
       " (u'Andrea', u'Kavanagh'),\n",
       " (u'Anthony', u'Henday'),\n",
       " (u'Antifaev', u'Jon'),\n",
       " (u'Approved', u'Footprint'),\n",
       " (u'Arthur', u'Chow'),\n",
       " (u'Aster', u'Cucine'),\n",
       " (u'Atefeh', u'Abbaspour'),\n",
       " (u'BARRY', u'COHEN'),\n",
       " (u'BEEN', u'AVAILABLE'),\n",
       " (u'BERNARD', u'LEUNG'),\n",
       " (u'BETTER', u'THAN'),\n",
       " (u'BRIGHT', u'VERSATILE'),\n",
       " (u'BY', u'RENOWNED'),\n",
       " (u'Barton', u'Myers'),\n",
       " (u'Bayview', u'Glen'),\n",
       " (u'Beachfront', u'Beauty'),\n",
       " (u'Berwick', u'whose'),\n",
       " (u'Bianco', u'Neve'),\n",
       " (u'Bio', u'Flame'),\n",
       " (u'Black', u'African'),\n",
       " (u'Blair', u'Gunn'),\n",
       " (u'Bob', u'Ferguson'),\n",
       " (u'Bows', u'archictects'),\n",
       " (u'Brian', u'Rybchinsky'),\n",
       " (u'Burn', u'Cktp'),\n",
       " (u'CANWEST', u'242'),\n",
       " (u'CARETAKER', u'ACC'),\n",
       " (u'CELLER', u'10X10'),\n",
       " (u'CERC', u'Relocation'),\n",
       " (u'CHARLENE', u'ALKERTON'),\n",
       " (u'CLASS', u'ART'),\n",
       " (u'COURT', u'ORDERED'),\n",
       " (u'COVETABLE', u'KERRISDALE'),\n",
       " (u'CP', u'Rail'),\n",
       " (u'CROWN', u'JEWEL'),\n",
       " (u'Cambrian', u'black'),\n",
       " (u'Cape', u'Cod'),\n",
       " (u'Careful', u'consideration'),\n",
       " (u'Carey', u'Mudford'),\n",
       " (u'Carissa', u'Siy'),\n",
       " (u'Carpe', u'diem'),\n",
       " (u'Casual', u'Gatherings'),\n",
       " (u'Cecilia', u'Xie'),\n",
       " (u'Celene', u'Cheang'),\n",
       " (u'Centennial', u'Parkway'),\n",
       " (u'Ch', u'u'),\n",
       " (u'Chebucto', u'Head'),\n",
       " (u'Childrens', u'Lottery'),\n",
       " (u'Christina', u'Watts'),\n",
       " (u'Circ', u'Drwy'),\n",
       " (u'Clarence', u'Debelle'),\n",
       " (u'Clay', u'Brickwork'),\n",
       " (u'Climate', u'Controlled'),\n",
       " (u'Coffee', u'Machine'),\n",
       " (u'Collectors', u'boaths'),\n",
       " (u'Communities', u'C1'),\n",
       " (u'Continental', u'Eastern'),\n",
       " (u'Craig', u'Doherty')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1 = ListTable()\n",
    "table1.append(['word1', 'word2'])\n",
    "for pmi in bigram_finder.nbest(bigram_measures.pmi, 100):\n",
    "    table1.append(pmi)\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>word1</td><td>word2</td><td>word3</td></tr><tr><td>+.</td><td>Learn</td><td>Success</td></tr><tr><td>0096</td><td>MARK</td><td>ROLLINS</td></tr><tr><td>0928</td><td>Prudential</td><td>Power</td></tr><tr><td>1716</td><td>Craig</td><td>Doherty</td></tr><tr><td>1726</td><td>JAMIE</td><td>BLAIR</td></tr><tr><td>2X16</td><td>GUEST</td><td>ROOM</td></tr><tr><td>Antifaev</td><td>Jon</td><td>Moss</td></tr><tr><td>BRIGHT</td><td>VERSATILE</td><td>BSMT</td></tr><tr><td>Berwick</td><td>whose</td><td>practice</td></tr><tr><td>CELLER</td><td>10X10</td><td>36X18</td></tr><tr><td>CERC</td><td>Relocation</td><td>Specialist</td></tr><tr><td>CHARLENE</td><td>ALKERTON</td><td>BROKER</td></tr><tr><td>COURT</td><td>ORDERED</td><td>SALE</td></tr><tr><td>COVETABLE</td><td>KERRISDALE</td><td>NEIGHBOURHOOD</td></tr><tr><td>CP</td><td>Rail</td><td>days</td></tr><tr><td>Ch</td><td>u</td><td>Shelly</td></tr><tr><td>Collectors</td><td>boaths</td><td>handsomely</td></tr><tr><td>Continental</td><td>Eastern</td><td>Seaboard</td></tr><tr><td>DAVID</td><td>ANDERSON</td><td>OEY</td></tr><tr><td>Effective</td><td>age</td><td>6yrs</td></tr><tr><td>Exciting</td><td>YouTube</td><td>HD</td></tr><tr><td>HOLDING</td><td>OR</td><td>BUILD</td></tr><tr><td>JAMES</td><td>MARTIN</td><td>MOLLOY</td></tr><tr><td>KRISTA</td><td>CHARLENE</td><td>ALKERTON</td></tr><tr><td>Keller</td><td>Williams</td><td>Capital</td></tr><tr><td>LIONS</td><td>GATE</td><td>BRIDGE</td></tr><tr><td>Learn</td><td>Success</td><td>blueprint</td></tr><tr><td>Lorne</td><td>Rose</td><td>Arch</td></tr><tr><td>Marco</td><td>Isle</td><td>reputed</td></tr><tr><td>NEVER</td><td>BEEN</td><td>AVAILABLE</td></tr><tr><td>OUTDOOR</td><td>RIDING</td><td>ARENAS</td></tr><tr><td>PAM</td><td>DECOURCEY</td><td>RRS</td></tr><tr><td>PAULA</td><td>PERRI</td><td>AMP</td></tr><tr><td>Relocation</td><td>Specialist</td><td>™</td></tr><tr><td>SO</td><td>MUCH</td><td>MORE</td></tr><tr><td>Shelves</td><td>Creates</td><td>Ambiance</td></tr><tr><td>Specialist</td><td>™</td><td>CERP</td></tr><tr><td>WORLD</td><td>CLASS</td><td>ART</td></tr><tr><td>Waterworks</td><td>freestanding</td><td>Empire</td></tr><tr><td>YouTube</td><td>HD</td><td>Video</td></tr><tr><td>basswood</td><td>int</td><td>.&</td></tr><tr><td>boaths</td><td>handsomely</td><td>housed</td></tr><tr><td>fr</td><td>CP</td><td>Rail</td></tr><tr><td>grande</td><td>piano</td><td>platform</td></tr><tr><td>honed</td><td>Cambrian</td><td>black</td></tr><tr><td>quieter</td><td>note</td><td>slip</td></tr><tr><td>raised</td><td>grande</td><td>piano</td></tr><tr><td>sheets</td><td>Bianco</td><td>Neve</td></tr><tr><td>.&</td><td>historic</td><td>wicker</td></tr><tr><td>1070</td><td>INNISFIL</td><td>BEACH</td></tr><tr><td>3732</td><td>Keller</td><td>Williams</td></tr><tr><td>4477</td><td>Josh</td><td>Nelson</td></tr><tr><td>7330</td><td>1039</td><td>McDonald</td></tr><tr><td>ALLOWED</td><td>FOR</td><td>LANDSCAPE</td></tr><tr><td>Aberdeen</td><td>Hall</td><td>Preparatory</td></tr><tr><td>BETTER</td><td>THAN</td><td>BRAND</td></tr><tr><td>BUILT</td><td>BY</td><td>RENOWNED</td></tr><tr><td>Black</td><td>African</td><td>Walnut</td></tr><tr><td>CERP</td><td>Certified</td><td>Negotiation</td></tr><tr><td>CLASS</td><td>ART</td><td>PIECE</td></tr><tr><td>Certified</td><td>Negotiation</td><td>Expert</td></tr><tr><td>Create</td><td>Magical</td><td>Oasis</td></tr><tr><td>DO</td><td>YOU</td><td>WANT</td></tr><tr><td>Dee</td><td>Taylor</td><td>Eustace</td></tr><tr><td>England</td><td>exude</td><td>classical</td></tr><tr><td>FOR</td><td>HOLDING</td><td>OR</td></tr><tr><td>FOUNTAIN</td><td>CREEK</td><td>ESTATES</td></tr><tr><td>Gold</td><td>medal</td><td>Tommie</td></tr><tr><td>Grounds</td><td>That</td><td>Expand</td></tr><tr><td>Lloyd</td><td>Wrights</td><td>Falling</td></tr><tr><td>Lutron</td><td>Graphic</td><td>Eye</td></tr><tr><td>MONIQUE</td><td>BERNADETTE</td><td>SIMANDL</td></tr><tr><td>Moffat</td><td>Colleen</td><td>Fisher</td></tr><tr><td>Negotiation</td><td>Expert</td><td>e</td></tr><tr><td>PLEASE</td><td>DO</td><td>NOT</td></tr><tr><td>PLUS</td><td>CARETAKER</td><td>ACC</td></tr><tr><td>Prudential</td><td>Power</td><td>Play</td></tr><tr><td>Stream</td><td>Christopher</td><td>Invidiata</td></tr><tr><td>Studio</td><td>Works</td><td>Architects</td></tr><tr><td>TWO</td><td>STUNNING</td><td>HOMES</td></tr><tr><td>Tastefuly</td><td>Renovated</td><td>3600</td></tr><tr><td>Tom</td><td>Westcott</td><td>e</td></tr><tr><td>WINE</td><td>CELLER</td><td>10X10</td></tr><tr><td>Walkup</td><td>Include</td><td>Rec</td></tr><tr><td>anticipated</td><td>yearly</td><td>production</td></tr><tr><td>apple</td><td>producing</td><td>orchard</td></tr><tr><td>conservatory</td><td>wh</td><td>ich</td></tr><tr><td>foam</td><td>insulation</td><td>underneath</td></tr><tr><td>gatherings</td><td>dinner</td><td>conversations</td></tr><tr><td>int</td><td>.&</td><td>historic</td></tr><tr><td>laser</td><td>cut</td><td>inlaid</td></tr><tr><td>leaving</td><td>maximum</td><td>clear</td></tr><tr><td>man</td><td>oil</td><td>camp</td></tr><tr><td>ping</td><td>pong</td><td>table</td></tr><tr><td>pnts</td><td>capturing</td><td>mesmerizing</td></tr><tr><td>wicker</td><td>clad</td><td>sunrm</td></tr><tr><td>yearly</td><td>production</td><td>increasing</td></tr><tr><td>™</td><td>CERP</td><td>Certified</td></tr><tr><td>3389</td><td>Yue</td><td>Yin</td></tr><tr><td>3776</td><td>Mathew</td><td>Poulsen</td></tr></table>"
      ],
      "text/plain": [
       "[['word1', 'word2', 'word3'],\n",
       " (u'+.', u'Learn', u'Success'),\n",
       " (u'0096', u'MARK', u'ROLLINS'),\n",
       " (u'0928', u'Prudential', u'Power'),\n",
       " (u'1716', u'Craig', u'Doherty'),\n",
       " (u'1726', u'JAMIE', u'BLAIR'),\n",
       " (u'2X16', u'GUEST', u'ROOM'),\n",
       " (u'Antifaev', u'Jon', u'Moss'),\n",
       " (u'BRIGHT', u'VERSATILE', u'BSMT'),\n",
       " (u'Berwick', u'whose', u'practice'),\n",
       " (u'CELLER', u'10X10', u'36X18'),\n",
       " (u'CERC', u'Relocation', u'Specialist'),\n",
       " (u'CHARLENE', u'ALKERTON', u'BROKER'),\n",
       " (u'COURT', u'ORDERED', u'SALE'),\n",
       " (u'COVETABLE', u'KERRISDALE', u'NEIGHBOURHOOD'),\n",
       " (u'CP', u'Rail', u'days'),\n",
       " (u'Ch', u'u', u'Shelly'),\n",
       " (u'Collectors', u'boaths', u'handsomely'),\n",
       " (u'Continental', u'Eastern', u'Seaboard'),\n",
       " (u'DAVID', u'ANDERSON', u'OEY'),\n",
       " (u'Effective', u'age', u'6yrs'),\n",
       " (u'Exciting', u'YouTube', u'HD'),\n",
       " (u'HOLDING', u'OR', u'BUILD'),\n",
       " (u'JAMES', u'MARTIN', u'MOLLOY'),\n",
       " (u'KRISTA', u'CHARLENE', u'ALKERTON'),\n",
       " (u'Keller', u'Williams', u'Capital'),\n",
       " (u'LIONS', u'GATE', u'BRIDGE'),\n",
       " (u'Learn', u'Success', u'blueprint'),\n",
       " (u'Lorne', u'Rose', u'Arch'),\n",
       " (u'Marco', u'Isle', u'reputed'),\n",
       " (u'NEVER', u'BEEN', u'AVAILABLE'),\n",
       " (u'OUTDOOR', u'RIDING', u'ARENAS'),\n",
       " (u'PAM', u'DECOURCEY', u'RRS'),\n",
       " (u'PAULA', u'PERRI', u'AMP'),\n",
       " (u'Relocation', u'Specialist', u'\\u2122'),\n",
       " (u'SO', u'MUCH', u'MORE'),\n",
       " (u'Shelves', u'Creates', u'Ambiance'),\n",
       " (u'Specialist', u'\\u2122', u'CERP'),\n",
       " (u'WORLD', u'CLASS', u'ART'),\n",
       " (u'Waterworks', u'freestanding', u'Empire'),\n",
       " (u'YouTube', u'HD', u'Video'),\n",
       " (u'basswood', u'int', u'.&'),\n",
       " (u'boaths', u'handsomely', u'housed'),\n",
       " (u'fr', u'CP', u'Rail'),\n",
       " (u'grande', u'piano', u'platform'),\n",
       " (u'honed', u'Cambrian', u'black'),\n",
       " (u'quieter', u'note', u'slip'),\n",
       " (u'raised', u'grande', u'piano'),\n",
       " (u'sheets', u'Bianco', u'Neve'),\n",
       " (u'.&', u'historic', u'wicker'),\n",
       " (u'1070', u'INNISFIL', u'BEACH'),\n",
       " (u'3732', u'Keller', u'Williams'),\n",
       " (u'4477', u'Josh', u'Nelson'),\n",
       " (u'7330', u'1039', u'McDonald'),\n",
       " (u'ALLOWED', u'FOR', u'LANDSCAPE'),\n",
       " (u'Aberdeen', u'Hall', u'Preparatory'),\n",
       " (u'BETTER', u'THAN', u'BRAND'),\n",
       " (u'BUILT', u'BY', u'RENOWNED'),\n",
       " (u'Black', u'African', u'Walnut'),\n",
       " (u'CERP', u'Certified', u'Negotiation'),\n",
       " (u'CLASS', u'ART', u'PIECE'),\n",
       " (u'Certified', u'Negotiation', u'Expert'),\n",
       " (u'Create', u'Magical', u'Oasis'),\n",
       " (u'DO', u'YOU', u'WANT'),\n",
       " (u'Dee', u'Taylor', u'Eustace'),\n",
       " (u'England', u'exude', u'classical'),\n",
       " (u'FOR', u'HOLDING', u'OR'),\n",
       " (u'FOUNTAIN', u'CREEK', u'ESTATES'),\n",
       " (u'Gold', u'medal', u'Tommie'),\n",
       " (u'Grounds', u'That', u'Expand'),\n",
       " (u'Lloyd', u'Wrights', u'Falling'),\n",
       " (u'Lutron', u'Graphic', u'Eye'),\n",
       " (u'MONIQUE', u'BERNADETTE', u'SIMANDL'),\n",
       " (u'Moffat', u'Colleen', u'Fisher'),\n",
       " (u'Negotiation', u'Expert', u'e'),\n",
       " (u'PLEASE', u'DO', u'NOT'),\n",
       " (u'PLUS', u'CARETAKER', u'ACC'),\n",
       " (u'Prudential', u'Power', u'Play'),\n",
       " (u'Stream', u'Christopher', u'Invidiata'),\n",
       " (u'Studio', u'Works', u'Architects'),\n",
       " (u'TWO', u'STUNNING', u'HOMES'),\n",
       " (u'Tastefuly', u'Renovated', u'3600'),\n",
       " (u'Tom', u'Westcott', u'e'),\n",
       " (u'WINE', u'CELLER', u'10X10'),\n",
       " (u'Walkup', u'Include', u'Rec'),\n",
       " (u'anticipated', u'yearly', u'production'),\n",
       " (u'apple', u'producing', u'orchard'),\n",
       " (u'conservatory', u'wh', u'ich'),\n",
       " (u'foam', u'insulation', u'underneath'),\n",
       " (u'gatherings', u'dinner', u'conversations'),\n",
       " (u'int', u'.&', u'historic'),\n",
       " (u'laser', u'cut', u'inlaid'),\n",
       " (u'leaving', u'maximum', u'clear'),\n",
       " (u'man', u'oil', u'camp'),\n",
       " (u'ping', u'pong', u'table'),\n",
       " (u'pnts', u'capturing', u'mesmerizing'),\n",
       " (u'wicker', u'clad', u'sunrm'),\n",
       " (u'yearly', u'production', u'increasing'),\n",
       " (u'\\u2122', u'CERP', u'Certified'),\n",
       " (u'3389', u'Yue', u'Yin'),\n",
       " (u'3776', u'Mathew', u'Poulsen')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = ListTable()\n",
    "table2.append(['word1', 'word2', 'word3'])\n",
    "for pmi in trigram_finder.nbest(trigram_measures.pmi, 100):\n",
    "    table2.append(pmi)\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.3. Use in Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
