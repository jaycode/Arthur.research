{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Documents With NLTK\n",
    "\n",
    "Let's explore the possibility of learning the documents by creating a corpus for them in python's NLTK package. This is important to help Arthur learns [collocations](http://www.nltk.org/howto/collocations.html) (expressions with multiple words), so it knows when to split tokens with more than one word.\n",
    "\n",
    "The goal here is to split this text, for example:\n",
    "\n",
    "```\n",
    "Finest property in New Brunswick! Modern, luxurious, architectural home takes full advantage of center stage on Lake Utopia in St. George. Panoramic views of the lake, 27,000 sq.ft.under roof and 100+ acres of unspoiled natural beauty. Experience resort-style living with 3 homes, 2 tournament quality outdoor tennis courts, and 1 stadium quality indoor tennis court with state-of-the-art indoor stadium lighting, water park including 2 pools & wading pool with umbrella feature, beach volleyball court, baseball field, custom go-kart track, driving range, indoor basketball court, playground, private dock with boat lift and 3 private beaches. Main home offers expanses of glass flooding the interior with brilliant light, sleek contemporary design, dramatic master suite with custom shower/central tub showcasing unparalleled views, master lanai with drapery screening and built-in Jacuzzi. Two guest homes provide luxurious privacy for visitors enjoying this exquisite estate. Welcome to paradise!\n",
    "```\n",
    "\n",
    "Into this:\n",
    "\n",
    "```\n",
    "[('finest', ), 'property', 'in', '']\n",
    "```\n",
    "\n",
    "Then with human aid turn it into concepts:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        name: 'location',\n",
    "        values: ['new brunswick', 'st. george'],\n",
    "        hints: ['in']\n",
    "    },\n",
    "    {\n",
    "        name: 'features',\n",
    "        values: ['modern', 'luxurious', 'architectural', 'tennis courts', 'built-in jacuzzi', ...]\n",
    "    },\n",
    "    {\n",
    "        name: 'bulding size',\n",
    "        values: ['27,000 sq.ft'],\n",
    "        hints: ['under roof']\n",
    "    },\n",
    "    {\n",
    "        name: 'land size',\n",
    "        values: ['100+ acres'],\n",
    "        hints: ['unspoiled natural beauty']\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "## 1. Collocations in prebuilt corpus\n",
    "\n",
    "First we will calculate collocations in a corpus that came with NLTK, just to see if this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Allon', u'Bacuth'),\n",
       " (u'Ashteroth', u'Karnaim'),\n",
       " (u'Ben', u'Ammi'),\n",
       " (u'En', u'Mishpat'),\n",
       " (u'Jegar', u'Sahadutha'),\n",
       " (u'Salt', u'Sea'),\n",
       " (u'Whoever', u'sheds'),\n",
       " (u'appoint', u'overseers'),\n",
       " (u'aromatic', u'resin'),\n",
       " (u'cutting', u'instrument')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.genesis.words('english-web.txt'))\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are corpus words stored and used anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'In', u'the', u'beginning', u'God', u'created', ...]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.genesis.words('english-web.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, turns out we simply need to pass tokenized words into `BigramCollocationFinder.from_words()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building our own corpus from plaintext\n",
    "\n",
    "In the above, we inputted `nltk.corpus.genesis.words` into `BigramCollocationFinder`. How do we create our own corpus from some plaintext files? Let's explore it below. This was all from [this stackoverflow discussion](http://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "This is a foo bar sentence.\n",
      "And this is the first txtfile in the corpus.\n",
      "2.txt\n",
      "Are you a foo bar? Yes I am. Possibly, everyone is.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# Let's create a corpus with 2 texts in different textfile.\n",
    "txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\n",
    "txt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\"\n",
    "corpus = [txt1,txt2]\n",
    "\n",
    "# Make new dir for the corpus.\n",
    "corpusdir = 'newcorpus/'\n",
    "if not os.path.isdir(corpusdir):\n",
    "    os.mkdir(corpusdir)\n",
    "\n",
    "# Output the files into the directory.\n",
    "filename = 0\n",
    "for text in corpus:\n",
    "    filename+=1\n",
    "    with open(corpusdir+str(filename)+'.txt','w') as fout:\n",
    "        print>>fout, text\n",
    "\n",
    "# Check that our corpus do exist and the files are correct.\n",
    "assert os.path.isdir(corpusdir)\n",
    "for infile, text in zip(sorted(os.listdir(corpusdir)),corpus):\n",
    "    assert open(corpusdir+infile,'r').read().strip() == text.strip()\n",
    "\n",
    "# Create a new corpus by specifying the parameters\n",
    "# (1) directory of the new corpus\n",
    "# (2) the fileids of the corpus\n",
    "# NOTE: in this case the fileids are simply the filenames.\n",
    "newcorpus = PlaintextCorpusReader('newcorpus/', '.*')\n",
    "\n",
    "import pdb\n",
    "# Access each file in the corpus.\n",
    "for infile in sorted(newcorpus.fileids()):\n",
    "    print infile # The fileids of each file.\n",
    "    fin = newcorpus.open(infile) # Opens the file.\n",
    "    print fin.read().strip() # Prints the content of the file\n",
    "    fin.close()\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a foo bar sentence.\n",
      "And this is the first txtfile in the corpus.\n",
      "Are you a foo bar? Yes I am. Possibly, everyone is.\n"
     ]
    }
   ],
   "source": [
    "# Access the plaintext; outputs pure string/basestring.\n",
    "print newcorpus.raw().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'This', u'is', u'a', u'foo', u'bar', u'sentence', u'.'], [u'And', u'this', u'is', u'the', u'first', u'txtfile', u'in', u'the', u'corpus', u'.']], [[u'Are', u'you', u'a', u'foo', u'bar', u'?'], [u'Yes', u'I', u'am', u'.'], [u'Possibly', u',', u'everyone', u'is', u'.']]]\n"
     ]
    }
   ],
   "source": [
    "# Access paragraphs in the corpus. (list of list of list of strings)\n",
    "# NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and \n",
    "#       nltk.tokenize.word_tokenize.\n",
    "#\n",
    "# Each element in the outermost list is a paragraph, and\n",
    "# Each paragraph contains sentence(s), and\n",
    "# Each sentence contains token(s)\n",
    "print newcorpus.paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'This', u'is', u'a', u'foo', u'bar', u'sentence', u'.'], [u'And', u'this', u'is', u'the', u'first', u'txtfile', u'in', u'the', u'corpus', u'.']]]\n"
     ]
    }
   ],
   "source": [
    "# To access pargraphs of a specific fileid.\n",
    "print newcorpus.paras(newcorpus.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'This', u'is', u'a', u'foo', u'bar', u'sentence', u'.'], [u'And', u'this', u'is', u'the', u'first', u'txtfile', u'in', u'the', u'corpus', u'.'], ...]\n"
     ]
    }
   ],
   "source": [
    "# Access sentences in the corpus. (list of list of strings)\n",
    "# NOTE: That the texts are flattened into sentences that contains tokens.\n",
    "print newcorpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'This', u'is', u'a', u'foo', u'bar', u'sentence', u'.'], [u'And', u'this', u'is', u'the', u'first', u'txtfile', u'in', u'the', u'corpus', u'.']]\n"
     ]
    }
   ],
   "source": [
    "# To access sentences of a specific fileid.\n",
    "print newcorpus.sents(newcorpus.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'This', u'is', u'a', u'foo', u'bar', u'sentence', ...]\n"
     ]
    }
   ],
   "source": [
    "# Access just tokens/words in the corpus. (list of strings)\n",
    "print newcorpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'This', u'is', u'a', u'foo', u'bar', u'sentence', ...]\n"
     ]
    }
   ],
   "source": [
    "# To access tokens of a specific fileid.\n",
    "print newcorpus.words(newcorpus.fileids()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collocations in custom corpus\n",
    "\n",
    "Putting the words from our custom corpus into `BigramAssocMeasures`, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u',', u'everyone'),\n",
       " (u'?', u'Yes'),\n",
       " (u'And', u'this'),\n",
       " (u'Are', u'you'),\n",
       " (u'I', u'am'),\n",
       " (u'Possibly', u','),\n",
       " (u'Yes', u'I'),\n",
       " (u'first', u'txtfile'),\n",
       " (u'txtfile', u'in'),\n",
       " (u'a', u'foo')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    newcorpus.words())\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newcorpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Next, we will look into implementing this with our documents.\n",
    "\n",
    "## 4. BigramAssocMeasures in extracted pdf documents\n",
    "\n",
    "We are going to follow the following pipeline:\n",
    "\n",
    "- Use ArthurReader to export documents into plaintexts.\n",
    "- Get words from them.\n",
    "- Use words to create collocations.\n",
    "- Use collocations in word splitting.\n",
    "\n",
    "The reason ArthurReader is needed when exporting documents is due to the bolded texts issue in pdf i.e. **A Text** would be written as \"A TextA TextA TextA Text\" if we just extract them as they are (when extracted from ArthurDocument, that is). Ideally later on we should keep bold information as feature, but for now let's just remove the duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Exporting to plaintexts\n",
    "\n",
    "This section attempts to extract documents and remove duplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparing required modules.\n",
    "import os\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "import sys\n",
    "import inspect\n",
    "base_path = os.path.realpath(\n",
    "    os.path.abspath(\n",
    "        os.path.join(\n",
    "            os.path.split(\n",
    "                inspect.getfile(\n",
    "                    inspect.currentframe()\n",
    "                )\n",
    "            )[0],\n",
    "            '..',\n",
    "            'Arthur.workspace'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "sys.path.append(base_path)\n",
    "\n",
    "class ListTable(list):\n",
    "    \"\"\" Overridden list class which takes a 2-dimensional list of \n",
    "        the form [[1,2,3],[4,5,6]], and renders an HTML Table in \n",
    "        IPython Notebook. \"\"\"\n",
    "    \n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col.encode('utf-8')))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from libs.arthur import ArthurDocument\n",
    "from zipfile import ZipFile\n",
    "from libs.arthur.errors import BatchReadingError\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# These are prototypes for ArthurReader's methods.\n",
    "\n",
    "def correct_block(block, return_details=False):\n",
    "    \"\"\"Corrects block elements.\n",
    "    \n",
    "    Args:\n",
    "        block(np.array): Block to correct\n",
    "        \n",
    "        return_details(bool): If True, return list instead of only corrected block. This list\n",
    "                              contains details needed for debugging:\n",
    "                              - removed features\n",
    "                              - added features\n",
    "                              Defaults to False\n",
    "    \"\"\"\n",
    "    fxid = ArthurDocument.get_feature_id('x')\n",
    "    fyid = ArthurDocument.get_feature_id('y')\n",
    "    positions = block[:,[fxid,fyid]]\n",
    "    tree = cKDTree(positions)\n",
    "\n",
    "    # Removes duplicate elements that are close together\n",
    "    radius = 0.4\n",
    "    neighbors = tree.query_ball_point(positions, radius)\n",
    "    neighbors = np.unique(neighbors)\n",
    "    # This returns numpy array like:\n",
    "    # [[0, 13, 26, 39] [1, 14, 27, 40] [5, 31, 44, 18] [11, 24, 37, 50]\n",
    "    # [16, 29, 42, 3] [17, 30, 43, 4] [21, 8, 34, 47] [22, 35, 48, 9]\n",
    "    # [32, 45, 19, 6] [36, 23, 10, 49] [38, 12, 25, 51] [41, 28, 2, 15]\n",
    "    # [46, 33, 7, 20] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62]\n",
    "    # [63] [64]]\n",
    "    #\n",
    "    # Which we will then remove duplicates e.g. remove index 13, 26, 39, 14, 27, etc.\n",
    "    removed = []\n",
    "    for n in neighbors:\n",
    "        removed.extend(np.sort(n)[1:])\n",
    "    \n",
    "    # Removes image elements\n",
    "    removed.extend(np.where(block[:,ArthurDocument.get_feature_id('img_width')] != -1)[0].tolist())\n",
    "    \n",
    "    cblock = np.delete(block, removed, axis=0)\n",
    "\n",
    "    if return_details:\n",
    "        return (cblock, removed)\n",
    "    else:\n",
    "        return cblock\n",
    "\n",
    "def get_texts(document):\n",
    "    \"\"\"Get corrected texts from a document.\n",
    "    Returns:\n",
    "        list: one text instance per block.\n",
    "    \"\"\"\n",
    "    blocks = DefaultCluster.create_blocks(document)\n",
    "\n",
    "    texts = []\n",
    "    for idx, block in enumerate(blocks):\n",
    "        cblock = correct_block(block)\n",
    "        texts.append(document.get_text(cblock))\n",
    "    return texts\n",
    "\n",
    "def create_corpus(zip_path, corpus_dir, batch_size=100, start_batch=0, stdout=None, recreate=False):\n",
    "    \"\"\"Create corpus from zip file. A corpus is basically just a list of text files.\n",
    "    Args:\n",
    "        zip_path(str):    Path of zip file to load.\n",
    "        corpus_dir(str):  Path to corpus dir where the files will be written into.\n",
    "        batch_size(int):  Size of batch to be processed. If we have one million documents,\n",
    "                          we'd want to process them in batches. Defaults to 100.\n",
    "        start_batch(int): When an error happens in batch processing, reader will return\n",
    "                          index of the last batch processed. enter that index value to\n",
    "                          start processing from that batch index. Defaults to 0.\n",
    "        stdout(Object):   Pass sys.stdout to print progress, or pass any object with `write`\n",
    "                          method to pass printed progress to it.\n",
    "        overwrite(bool):  Overwrite files as they are created?\n",
    "    \"\"\"\n",
    "    zipfile = ZipFile(zip_path, 'r')\n",
    "    namelist = zipfile.namelist()\n",
    "    jobs_total = len(namelist)\n",
    "    jobs_left = jobs_total - start_batch*batch_size\n",
    "    \n",
    "    def process_batch(zipfile, corpus_dir, batch, total, counter=0, overwrite=False):\n",
    "        for docname in batch:\n",
    "            counter += 1\n",
    "            filename = os.path.join(corpus_dir, docname+'.txt')\n",
    "            if os.path.isfile(filename) and not overwrite:\n",
    "                stdout.write(\"%s already exists (%i/%i)\\n\" % (docname, counter, total))\n",
    "            else:\n",
    "                content = zipfile.read(docname)\n",
    "                stdout.write(\"processing %s (%i/%i)\\n\" % (docname, counter, total))\n",
    "                document = ArthurDocument(content, name=docname)\n",
    "                texts = get_texts(document)\n",
    "                if len(texts) > 0:\n",
    "                    if not os.path.isdir(corpus_dir):\n",
    "                        os.mkdir(corpus_dir)\n",
    "\n",
    "                    with open(filename,'w') as fout:\n",
    "                        for text in texts:\n",
    "                            print>>fout, text\n",
    "                else:\n",
    "                    stdout.write(\"    empty text! moving on...\\n\")\n",
    "\n",
    "    while jobs_left > 0:\n",
    "        job_start = jobs_total - jobs_left\n",
    "        job_end = job_start + batch_size\n",
    "        batch = namelist[job_start:job_end]\n",
    "        process_batch(zipfile, corpus_dir, batch, jobs_total, job_start)\n",
    "        jobs_left -= batch_size\n",
    "\n",
    "    zipfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'overwrite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ef14009bf201>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcorpus_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'corpus'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mcreate_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mBatchReadingError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-1c66e7354df6>\u001b[0m in \u001b[0;36mcreate_corpus\u001b[1;34m(zip_path, corpus_dir, batch_size, start_batch, stdout, recreate)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mjob_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob_start\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnamelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjob_end\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mprocess_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjobs_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mjobs_left\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-1c66e7354df6>\u001b[0m in \u001b[0;36mprocess_batch\u001b[1;34m(zipfile, corpus_dir, batch, total, counter)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s already exists (%i/%i)\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdocname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'overwrite' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.collocations import *\n",
    "from libs.arthur.errors import BatchReadingError\n",
    "\n",
    "zip_path = 'DRAS_sample_v1_20150605.zip'\n",
    "corpus_dir = 'corpus'\n",
    "try:\n",
    "    create_corpus(zip_path, corpus_dir, stdout=sys.stdout)        \n",
    "except BatchReadingError as e:\n",
    "    print(e.msg)\n",
    "    print('last batch was: %i' % e.last_batch)\n",
    "    # Maybe do some retries here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Collocations\n",
    "\n",
    "Here's the plan:\n",
    "1. Find collocations in our custom corpus.\n",
    "2. Use them to tokenize a document's content with [MWETokenizer](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.mwe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(corpus_dir, '.*')\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_words(corpus.words())\n",
    "trigram_finder = TrigramCollocationFinder.from_words(corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table1 = ListTable()\n",
    "table1.append(['word1', 'word2'])\n",
    "for pmi in bigram_finder.nbest(bigram_measures.pmi, 10):\n",
    "    table1.append(pmi)\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table2 = ListTable()\n",
    "table2.append(['word1', 'word2', 'word3'])\n",
    "for pmi in trigram_finder.nbest(trigram_measures.pmi, 10):\n",
    "    table2.append(pmi)\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.3. Tokenize A Document with MWETokenizer\n",
    "\n",
    "The next step is to tokenize our document, extracting all the data fields from them. Each \n",
    "\n",
    "\n",
    "keep the results of these bigrams and trigrams, then use them for MWETokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "tokenizer = MWETokenizer(bigram_finder.nbest(bigram_measures.pmi, 30000))\n",
    "\n",
    "pdf_path = os.path.join(base_path, 'exploration', 'pdfs', '10086622.pdf')\n",
    "with open(pdf_path, 'r') as f:\n",
    "    document = ArthurDocument(f.read())\n",
    "    texts = get_texts(document)\n",
    "    text = ' '.join(texts)\n",
    "    tokenized = tokenizer.tokenize(word_tokenize(text))\n",
    "    print tokenized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
